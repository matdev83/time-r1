
# Timeâ€‘R1Â ImplementationÂ Plan

*Generated on 2025-06-15T20:21:59Z*

---

## 1Â Â Quick Onâ€‘Boarding for CodingÂ Agents

| Key | Value |
|-----|-------|
| **Objective** | Build a complete, reproducible Python project that trains, evaluates, and serves the **Timeâ€‘R1** foundation model for timeâ€‘series forecasting |
| **Stages** | PromptÂ Templating â†’ SFTÂ Warmâ€‘Up â†’ GRIPÂ RL â†’ Slowâ€‘ThinkingÂ Inference â†’ Evaluation/Serving |
| **Stack** | PythonÂ â‰¥Â 3.10 Â·Â PyTorchÂ â‰¥Â 2.2 Â·Â ðŸ¤—Â TransformersÂ /Â PEFT Â·Â trl Â·Â FastAPI Â·Â LightningÂ CLI Â·Â MLflow or WeightsÂ &Â Biases |
| **Standards** | **TDD** (pytest+coverage), **SOLID**, **DRY**, **KISS** |
| **Tooling** | `poetry`Â (env+build), `black`Â (fmt), `isort`, `ruff`, `mypy`, `preâ€‘commit`, GitHubÂ ActionsÂ CI |
| **Acceptance Gate** | All tests pass Â· QA metrics meet thresholds Â· CI/CD pipeline green |

---

## 2Â Â RepositoryÂ Skeleton

```text
time-r1/
â”œâ”€â”€ data/                 # Raw & processed datasets (gitâ€‘ignored)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ time_r1/              # Source package
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ config/           # YAML & pydantic settings
â”‚       â”œâ”€â”€ datasets/         # DataModule implementations
â”‚       â”œâ”€â”€ prompts/          # Jinja2 templates & builders
â”‚       â”œâ”€â”€ models/           # Model wrappers
â”‚       â”‚   â”œâ”€â”€ sft/
â”‚       â”‚   â””â”€â”€ grip/
â”‚       â”œâ”€â”€ training/         # Training loops / LightningTasks
â”‚       â”œâ”€â”€ inference/        # Slowâ€‘thinking engine & APIs
â”‚       â”œâ”€â”€ evaluation/       # Metrics & benchmark runners
â”‚       â””â”€â”€ utils/
â”œâ”€â”€ scripts/              # CLI entryâ€‘points
â”œâ”€â”€ tests/                # PyTest suites (mirrors src tree)
â”œâ”€â”€ docs/                 # Sphinx + MkDocs
â”œâ”€â”€ pyproject.toml        # Build + dev dependencies
â””â”€â”€ README.md
```

---

## 3Â Â Highâ€‘Level Workflow

```mermaid
flowchart LR
    subgraph TrainingÂ Pipeline
        P[Prompt Template Builder] --> SFT[SFT Warmâ€‘Up]
        SFT --> GRIP[GRIPÂ RL\n(Guided Reward Improvement)]
    end
    GRIP -->|checkpoint| M[Timeâ€‘R1Â Weights]
    subgraph InferenceÂ &Â Serving
        M --> ST[Slowâ€‘Thinking Executor]
        ST --> R[Forecast Results]
    end
    ST -->|metrics| E[EvaluationÂ Suite]
```

---

## 4Â Â Task &Â Subâ€‘TaskÂ Breakâ€‘Down

> **Notation**  
> **I**Â =Â Inputsâ€ƒâ€¢â€ƒ**D**Â =Â Deliverablesâ€ƒâ€¢â€ƒ**T**Â =Â Testsâ€ƒâ€¢â€ƒ**A**Â =Â Acceptance Criteria

### **Tâ€‘0Â Â ProjectÂ Bootstrap**

- **Goal**â€ƒEstablish scalable, testâ€‘first repo with automated CI.
- **I**Â ðŸ—…Â â€”  
- **D**Â `pyproject.toml`, GitHubÂ Actions, `preâ€‘commit` hooks.  
- **T**Â `pytest -q`, flake/ruff, mypy.  
- **A**Â CI workflow passes on push & PR.

---

### **Tâ€‘1Â Â DatasetÂ IngestionÂ &Â Preâ€‘Processing**

- **Goal**â€ƒPrepare diverse time series datasets for training and evaluation, ensuring data quality and accessibility.
- **I**â€ƒRaw time series datasets (e.g., ETT, Exchange, Wind, AQ, NASDAQ as mentioned in the paper).
- **D**â€ƒProcessed `*.parquet` files, PyTorch LightningDataModule.
- **T**â€ƒChecksum tests, schema validation unit tests, statistical drift tests.
- **A**â€ƒFiles exist, schema OK, $\le 1e-3$ diff in statistical drift, DataModule passes smoke train-loop.
- **Sub-Tasks**:

    | Subâ€‘Task | I | D | T | A |
    |----------|---|---|---|---|
    | **1.1 Downloaders** | dataset listÂ (JSON) | `*.csv` raw | checksum test | files exist |
    | **1.2 SchemaÂ Validation** | rawÂ csv | `pydantic` schemas | unit tests | schema OK |
    | **1.3 FeatureÂ Engineering** | raw | `*.parquet` processed | statistical drift test | â‰¤1eâ€‘3 diff |
    | **1.4 DataModule** | processed | PyTorch LightningDataModule | smoke trainâ€‘loop |  âœ” |

---

### **Tâ€‘2Â Â PromptÂ TemplateÂ Builder**

- **Goal**â€ƒDevelop a standardized instructional framework for LLM inputs, encoding task-specific knowledge.
- **I**â€ƒTask specification, data schema, Jinja2 templates.
- **D**â€ƒ`src/time_r1/prompts/PromptTemplate.py` class, YAML files for Jinja templates.
- **T**â€ƒRender snapshot tests, validation against expected output formats.
- **A**â€ƒ100% deterministic render for fixed seed; adheres to paper's "Training Template" components.
- **Implementation Details**:
  - The `PromptTemplate` class should encapsulate the five components described in the paper's "Training Template" section:
        1. **Task Definition**: Establish objectives and problem scope.
        2. **Dataset Description**: Specify temporal characteristics and application scenarios.
        3. **Channel Information**: Delineate input signal types.
        4. **Testing Data**: Provide timestamps and historical series.
        5. **Format Instruction**: Define output templates (e.g., `<think>`, `<answer>` tags).
  - Use Jinja2 for flexible template rendering, allowing dynamic insertion of historical data and task-specific instructions.

---

### **Tâ€‘3Â Â Supervised Fineâ€‘TuningÂ (SFT)**

- **Goal**â€ƒWarm-up adaptation of the LLM for time series forecasting, ensuring stable training and proper output formatting.
- **I**â€ƒProcessed time series data, base LLM (e.g., Qwen2.5-7B-Instruct).
- **D**â€ƒHugging Face Dataset object, `lora_config.json`, `sft.pt` checkpoint.
- **T**â€ƒSample shape tests, parameter count tests, regression tests (loss < $\tau$).
- **A**â€ƒDataset samples have correct shape, LoRA config applies with minimal parameter diff, training loss $\le \tau=0.05$, W&B run exists.
- **Sub-Tasks**:

    | Subâ€‘Task | I | D | T | A |
    |----------|---|---|---|---|
    | **3.1 SFT Data Construction** | processed data | HF Dataset object | sample shape test | len>0 |
    | **3.1.1 Initial Prediction Generation** | historical data | candidate predictions | unit tests | DeepSeek-R1 generates $k$ predictions |
    | **3.1.2 Optimal Prediction Selection** | candidate predictions, ground truth | best prediction | MAPE test | selects min MAPE prediction |
    | **3.1.3 CoT Refinement** | historical data, true prediction, CoT of best pred | refined CoT | unit tests | CoT aligns with ground truth |
    | **3.1.4 Structured Data Creation** | refined CoT, true prediction | SFT training sample | format test | `<think>`, `<answer>` tags present |
    | **3.2 LoRA Config Generation** | base LLM id | `lora_config.json` | parameter count test | diff<1% |
    | **3.3 Trainer Script Implementation** | 3.1 + 3.2 | ckpt `sft.pt` | regression test (loss<Ï„) | loss $\le \tau=0.05$ |
    | **3.4 Logging & Experiment Tracking** | training metrics | W&B run | CI artifact test | run exists, metrics logged |

- **Implementation Details**:
  - The SFT data construction should follow the three key steps outlined in the paper's "Supervised Fine-tuning for Warmup Adaptation" section and Algorithm 1:
        1. Leverage DeepSeek-R1 (or similar LLM) to generate initial time-series predictions on the training set with strict formatting.
        2. Select the optimal prediction for each sample based on the Mean Absolute Percentage Error (MAPE) metric.
        3. Inject the true prediction value and the high-quality CoT from the previous step back into the LLM as prompts to synthesize a revised CoT that logically culminates in the correct prediction.
        4. Concatenate the refined CoT and the true prediction value, demarcating the final answer using `<answer>` tags and reasoning with `<think>` tags to create structured training data.
  - Perform a single-epoch fine-tuning with a small learning rate (e.g., 5e-5 as in the paper).

---

### **Tâ€‘4Â Â GRIPÂ RLÂ Stage**

- **Goal**â€ƒRefine the SFT model for generalization and slow-thinking capabilities using reinforcement learning, guided by fine-grained, multi-objective rewards.
- **I**â€ƒ`sft.pt` (SFT-trained model), time series data, reward function components.
- **D**â€ƒ`src/time_r1/training/grip_trainer.py`, `grip.pt` (final GRIP-trained model).
- **T**â€ƒParameterized reward tests, convergence tests (KL divergence, performance metrics), unit tests for checkpointing and safety filters.
- **A**â€ƒReward $\in [-1,1]$, KL < 0.1, performance improvement over SFT, top-k checkpoints saved, toxic output rate $\le 0.2\%$.
- **Sub-Tasks**:

    | Subâ€‘Task | I | D | T | A |
    |----------|---|---|---|---|
    | **4.1 Reward Function Design** | ground-truth, prediction | `src/time_r1/training/reward.py` | parameterized tests | reward $\in [-1,1]$ |
    | **4.1.1 Format Rewards** | model output | `gamma_Format`, `gamma_Length` | unit tests | syntactic validity, completeness |
    | **4.1.1.1 Format Reward ($\gamma_{\text{Format}}$)** | model output | binary penalty | unit tests | Ensures `<think>`, `<answer>` tags and proper structure. $\gamma_{\text{Format}} = 0$ if valid, $-1$ otherwise. |
    | **4.1.1.2 Length Reward ($\gamma_{\text{Length}}$)** | generated sequence length, ground truth length | positive feedback | unit tests | Encourages full sequence generation. $\gamma_{\text{Length}} = 0.1$ if $\text{len(answer)} \ge \text{len(ground\_truth)}$, else $0.1 \cdot \frac{\text{len(answer)}}{\text{len(ground\_truth)}}$. |
    | **4.1.2 Accuracy Rewards** | normalized pred/target | `gamma_MSE`, `gamma_Seasonal`, `gamma_Trend` | unit tests | numerical precision, temporal fidelity |
    | **4.1.2.1 MSE Reward ($\gamma_{\text{MSE}}$)** | normalized prediction, target | bounded reward signal | unit tests | Assesses numerical precision. $\gamma_{\text{MSE}} = \left(1 - \frac{1}{1 + e^{-0.3 \cdot \text{MSE}}}\right) \cdot 2$. |
    | **4.1.2.2 Seasonal-Trend Decomposition Rewards ($\gamma_{\text{Seasonal}}, \gamma_{\text{Trend}}$)** | predicted/true sequences | separate MSE terms | unit tests | Captures temporal structure. $\gamma_{\text{Seasonal}} = \frac{1}{n} \sum_{i=1}^{n} \left(s_i^{\text{true}} - s_i^{\text{pred}}\right)^2$, $\gamma_{\text{Trend}} = \frac{1}{n} \sum_{i=1}^{n} \left(t_i^{\text{true}} - t_i^{\text{pred}}\right)^2$. |
    | **4.1.3 Structural Similarity Reward ($\gamma_{\text{CP}}$)** | predicted/ground-truth extrema | credit for matches | unit tests | Ensures change-point capture. $\gamma_{\text{CP}} = \left(\frac{N_{\text{cmax}}}{N_{\text{gmax}}} \cdot 0.2 \right) + \left(\frac{N_{\text{cmin}}}{N_{\text{gmin}}} \cdot 0.2\right)$. |
    | **4.2 GRIP Trainer Implementation** | sft.ckpt, reward fn | `src/time_r1/training/grip_trainer.py`, `grip.pt` | convergence test | KL < 0.1, performance improvement |
    | **4.2.1 Non-uniform Sampling Strategy** | $k \cdot G$ candidates | `src/time_r1/training/sampling_strategy.py` | unit tests | Balances exploration/exploitation. Implement "Local Random Sampling" and "Cluster-based Random Sampling" as described in the paper. |
    | **4.2.2 Adaptive Weighting for Gradient Enhancement** | trajectory rewards | `src/time_r1/training/adaptive_weighting.py` | unit tests | Amplifies high-quality gradients. Implement $w_i^U = \frac{\exp(\hat{x}_{q, o_i})}{\sum_{j=1}^{G} \exp(\hat{x}_{q, o_j})}$, where $\hat{x}_{q, o_i} = R(o_i)$. |
    | **4.3 Checkpointing** | training state | Best ckpt tags | unit test | top-k saved, reproducible |
    | **4.4 Safety Filters** | policy outputs | `src/time_r1/inference/filters.py` | toxic rate test | $\le 0.2\%$ toxic output |

- **Implementation Details**:
  - The overall reward for RL training is the sum of the above rewards.
  - Implement GRIP objective function as formalized in Equation 6 of the paper, integrating non-uniform sampling and adaptive weighting.
  - Use hyperparameters $\epsilon=0.2$ and $\beta=0.04$ (as per paper's experimental setup).
  - Group size $G=16$, $k=3$ for candidate trajectories.
  - Batch size 16, learning rate 1e-6, policy temperature 1, max completion length 3000.
  - Training on a multi-GPU setup (e.g., 4-GPU A800 cluster as in the paper).

---

### **Tâ€‘5Â Â Slowâ€‘ThinkingÂ InferenceÂ Engine**

- **Goal**â€ƒImplement the multi-step reasoning engine for Time-R1 inference.
- **Sub-Steps**: **draft â†’ reflect â†’ refine** (iterative process).
- **I**â€ƒ`grip.pt` (trained GRIP model), input prompt, historical time series data.
- **D**â€ƒ`src/time_r1/inference/slow_think.py` (modular chain API), `src/time_r1/inference/executor.py` (inference executor).
- **T**â€ƒGold forecast vs baseline, latency benchmarks.
- **A**â€ƒMASE improvement $\ge 10\%$ over SFT baseline; inference latency $\le 500$ ms (A100 GPU).
- **Implementation Details**:
  - **5.1 Draft Module**: Generates initial forecast based on prompt and historical data.
  - **5.2 Reflect Module**: Evaluates the draft, identifies inconsistencies or areas for improvement (e.g., using a critic model or rule-based checks).
  - **5.3 Refine Module**: Adjusts the forecast based on reflection, potentially re-prompting the model or applying post-processing.
  - **5.4 Chain Orchestration**: Implement a flexible API (e.g., using a custom pipeline or a framework like LangChain) to orchestrate these modules iteratively until a satisfactory forecast is achieved or a maximum number of iterations is reached.

---

### **Tâ€‘6Â Â EvaluationÂ &Â BenchmarkÂ Harness**

- Metrics: **MSE**, **MAE**, **MAPE**, **RÂ²**, **NRMSE**.  
- **I**Â model, testÂ sets.  
- **D**Â `eval_cli.py`, pytest fixtures.  
- **T**Â metric reproducibility.  
- **A**Â CI run outputs JSON report.

---

### **Tâ€‘7Â Â APIÂ &Â Service Layer**

| Subâ€‘Task | I | D | T | A |
|----------|---|---|---|---|
| **7.1 FastAPI App** | slow_think.py | `main.py` | `/health` e2e | 200 OK |
| **7.2 Dockerfile** | repo | image | docker run test | inf latency < 2Ã—RT |
| **7.3 Helm Chart (opt.)** | image | chart/ | kind test | pod ready |

---

### **Tâ€‘8Â Â ExperimentÂ Tracking &Â Reproducibility**

- **I**Â configÂ dicts.  
- **D**Â `conf/` YAML + seeder.  
- **T**Â hashâ€‘match test.  
- **A**Â run can be reproduced via `make replay HASH=â€¦`.

---

### **Tâ€‘9Â Â MLOpsÂ &Â ProductionÂ Readiness**

- **Goal**â€ƒEnsure robust, scalable, and secure deployment of the Time-R1 model.
- **Sub-Tasks**:

    | Subâ€‘Task | I | D | T | A |
    |----------|---|---|---|---|
    | **9.1 Data Versioning** | raw/processed data | DVC/MLflow artifacts | checksum test | data integrity |
    | **9.2 Model Registry** | `sft.pt`, `grip.pt` | MLflow Model Registry | API access test | model versioned |
    | **9.3 Monitoring & Alerting** | inference logs, metrics | Prometheus/Grafana setup | alert trigger test | anomaly detected |
    | **9.4 Security Hardening** | FastAPI app, Dockerfile | security best practices | penetration test | no critical vulns |
    | **9.5 Scalability Design** | traffic patterns | auto-scaling config | load test | handles peak load |

---

### **Tâ€‘10Â Â Documentation**

- SphinxÂ `autodoc`, MkDocs site.  
- Smoke build test in CI.

---

## 5Â Â Deliverables Checklist

- ðŸ—¹Â `time_r1` Python package + unit tests (>90â€¯% coverage)  
- ðŸ—¹Â Preâ€‘trained checkpoints (`sft.pt`, `grip.pt`)  
- ðŸ—¹Â Docker image pushed to registry  
- ðŸ—¹Â Minimal API demonstration notebook  
- ðŸ—¹Â Comprehensive docs site at `/docs`

---

## 6Â Â AcceptanceÂ Gate Summary

| Metric | Threshold |
|--------|-----------|
| **Codeâ€‘cov** | â‰¥â€¯90â€¯% |
| **Train loss Î”** | âˆ’tocvx until â‰¤â€¯0.05 |
| **Forecast MASE** | Better than SOTA baseline |
| **Latency (128Â steps)** | â‰¤â€¯500â€¯ms GPUÂ A100 |
| **CI Pass Rate** | 100â€¯% |

---

### HappyÂ codingÂ ðŸ¤–
